{
  "subject": "Machine Learning",
  "topics": [
    "Introduction to Machine Learning",
    "Data Pre-processing and understanding",
    "Clustering",
    "Classification Techniques",
    "Bayesian Estimation",
    "Hidden Markov Models",
    "Reinforcement learning"
  ],
  "subtopics": [
    "supervised learning",
    "learning multiple classes",
    "regression",
    "model selection and generalization",
    "Bayesian decision theory",
    "losses and risks",
    "discriminant functions",
    "utility theory",
    "association rules",
    "parametric methods",
    "maximum likelihood estimation",
    "bias and variance",
    "multivariate data",
    "parameter estimation",
    "estimating missing values",
    "multivariate normal distribution",
    "multivariate classification and regression",
    "dimensionality reduction",
    "subset selection",
    "PCA",
    "LDA",
    "Isomaps",
    "LLE",
    "mixture densities",
    "K-means algorithm",
    "EM-algorithm",
    "hierarchical clustering",
    "choosing number of clusters",
    "Non-parametric methods",
    "non-parametric density estimation",
    "non-parametric classification and regression",
    "decision trees",
    "pruning",
    "rule extraction from trees",
    "learning rules from data",
    "Linear discrimination (two classes, multiple classes, pairwise separation)",
    "Perceptrons",
    "multilayer perceptrons",
    "backpropagation algorithm",
    "training procedures and network tuning",
    "Competitive learning",
    "Radial basis functions",
    "Incorporating rule based knowledge",
    "Kernel machines",
    "hyperplanes",
    "SVM",
    "kernel trick",
    "kernel machines for regression",
    "kernel dimensionality reduction",
    "estimating parameter of Distributions and Functions",
    "Graphical models",
    "conditional independence",
    "d-separation",
    "belief propagation",
    "Markov random fields",
    "learning the structure of a graphical model",
    "influence diagrams",
    "discrete markov processes",
    "HMM",
    "three problems of HMM",
    "evaluation problem",
    "finding state sequence",
    "learning model parameters",
    "HMM with input",
    "single state case",
    "elements of reinforcement learning",
    "model based learning",
    "temporal difference learning",
    "generalization",
    "partially observable states",
    "Combining multiple learners",
    "model combinations schemes",
    "voting",
    "error-correcting output codes",
    "bagging",
    "boosting",
    "cascading"
  ],
  "competencies": [
    "Understand the theoretical concepts of machine learning.",
    "Develop working knowledge of state-of-the-art techniques in machine learning."
  ],
  "learning_outcomes": [
    "Understand fundamental concepts of supervised learning, regression, model selection, generalization, Bayesian decision theory, and association rules.",
    "Apply data pre-processing techniques, parametric methods like maximum likelihood estimation, and analyze bias-variance trade-offs.",
    "Perform dimensionality reduction using techniques such as PCA, LDA, Isomaps, and LLE.",
    "Implement various clustering algorithms including K-means, EM-algorithm, and hierarchical clustering, and apply non-parametric methods.",
    "Develop and evaluate classification models using decision trees, linear discrimination, perceptrons, multilayer perceptrons, and backpropagation.",
    "Utilize advanced classification techniques such as competitive learning, Radial Basis Functions, Kernel machines, and Support Vector Machines (SVMs).",
    "Apply Bayesian estimation techniques and understand graphical models, conditional independence, and belief propagation.",
    "Analyze and solve problems using Hidden Markov Models (HMMs), including evaluation, state sequence finding, and parameter learning.",
    "Grasp the principles of reinforcement learning, including model-based and temporal difference learning, and methods for combining multiple learners like bagging and boosting."
  ]
}